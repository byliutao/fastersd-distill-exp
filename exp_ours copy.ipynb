{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import clip\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import json\n",
    "from generate import generate_single_image, load_model\n",
    "from dataclasses import dataclass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TestConfig:\n",
    "    device = \"cuda:1\"\n",
    "    ours_steps = 8\n",
    "    hpsv2_sub_dirs = [\"anime\",\"photo\",\"concept-art\",\"paintings\"]\n",
    "    ours_hpsv2_path = \"/data/liutao/data/ours_44k_8s_kl_hpsv2\"\n",
    "    ours_coco_path = \"/data/liutao/data/ours_44k_8s_kl_coco\"\n",
    "    clip_model_id = \"ViT-L/14@336px\"\n",
    "    ours_model_id = \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints_klloss/vsd_global_step44000_8nis_kl.pth\"\n",
    "    ours_base_path = \"/data/\"\n",
    "    coco_caption_path = \"/data/dataset/coco2014-val/annotations/captions_val2014.json\"\n",
    "    caption_num = 30000\n",
    "    seed = 2024\n",
    "config = TestConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(config.ours_coco_path):\n",
    "    os.makedirs(config.ours_coco_path)\n",
    "if not os.path.exists(config.ours_hpsv2_path):\n",
    "    os.makedirs(config.ours_hpsv2_path)\n",
    "for sub_dir in config.hpsv2_sub_dirs:\n",
    "    sub_dir_path = os.path.join(config.ours_hpsv2_path,sub_dir)\n",
    "    if not os.path.exists(sub_dir_path):\n",
    "        os.makedirs(sub_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "clip_model, clip_preprocess = clip.load(config.clip_model_id)\n",
    "clip_model = clip_model.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(config.ours_base_path, config.ours_model_id, config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text], truncate=True)\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    image_input = image_input.to(config.device)\n",
    "    text_input = text_input.to(config.device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coco30k_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 A bicycle replica with a clock as the front wheel. A black Honda motorcycle parked in front of a garage.\n"
     ]
    }
   ],
   "source": [
    "coco_f = open(config.coco_caption_path)\n",
    "coco_annotations = json.load(coco_f)\n",
    "captions = []\n",
    "for annotation in coco_annotations['annotations']:\n",
    "    caption = annotation['caption']\n",
    "    captions.append(caption)\n",
    "coco_f.close()\n",
    "random.seed(config.seed)\n",
    "captions_30k = random.choices(captions, k=config.caption_num)\n",
    "print(len(captions_30k),captions[0],captions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current num: 1000 current avg clip score: 0.2618109130859375\n",
      "current num: 2000 current avg clip score: 0.26182708740234373\n",
      "current num: 3000 current avg clip score: 0.26194840494791666\n",
      "current num: 4000 current avg clip score: 0.26146597290039064\n",
      "current num: 5000 current avg clip score: 0.26180556640625\n",
      "current num: 6000 current avg clip score: 0.26171663411458335\n",
      "current num: 7000 current avg clip score: 0.2617603236607143\n",
      "current num: 8000 current avg clip score: 0.26200048828125\n",
      "current num: 9000 current avg clip score: 0.2617371554904514\n",
      "current num: 10000 current avg clip score: 0.2616024291992187\n",
      "current num: 11000 current avg clip score: 0.2615211181640625\n",
      "current num: 12000 current avg clip score: 0.2614950968424479\n",
      "current num: 13000 current avg clip score: 0.2614385235126202\n",
      "current num: 14000 current avg clip score: 0.2613802228655134\n",
      "current num: 15000 current avg clip score: 0.2614111490885417\n",
      "current num: 16000 current avg clip score: 0.26129731750488283\n",
      "current num: 17000 current avg clip score: 0.2613666633157169\n",
      "current num: 18000 current avg clip score: 0.2614095255533854\n",
      "current num: 19000 current avg clip score: 0.26138991185238486\n",
      "current num: 20000 current avg clip score: 0.26138226318359375\n",
      "current num: 21000 current avg clip score: 0.2613655482700893\n",
      "current num: 22000 current avg clip score: 0.2613635586825284\n",
      "current num: 23000 current avg clip score: 0.26133377207880437\n",
      "current num: 24000 current avg clip score: 0.2613039347330729\n",
      "current num: 25000 current avg clip score: 0.2613473046875\n",
      "current num: 26000 current avg clip score: 0.2613263455904447\n",
      "current num: 27000 current avg clip score: 0.2612843108000579\n",
      "current num: 28000 current avg clip score: 0.2612552969796317\n",
      "current num: 29000 current avg clip score: 0.2612930487271013\n",
      "current num: 30000 current avg clip score: 0.2612726806640625\n",
      "AVG CLIP Score: 0.2612726806640625\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, prompt in enumerate(captions_30k):\n",
    "    image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024,num_inference_steps=config.ours_steps)\n",
    "    score = get_clip_score(image, prompt)\n",
    "    save_name = str(count)+\".jpg\"\n",
    "    image.save(os.path.join(config.ours_coco_path,save_name))\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hpsv2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get repository contents: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/datasets/zhwang/HPDv2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fe1dd777070>, 'Connection to huggingface.co timed out. (connect timeout=None)'))\n"
     ]
    }
   ],
   "source": [
    "import hpsv2\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024,num_inference_steps=config.ours_steps)\n",
    "        image.save(os.path.join(config.ours_hpsv2_path, style, f\"{idx:05d}.jpg\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n",
      "-----------benchmark score ---------------- \n",
      "ours_44k_8s_kl_hpsv2 paintings       26.11 \t 0.1047\n",
      "ours_44k_8s_kl_hpsv2 anime           26.46 \t 0.1735\n",
      "ours_44k_8s_kl_hpsv2 photo           26.39 \t 0.2102\n",
      "ours_44k_8s_kl_hpsv2 concept-art     26.00 \t 0.1058\n"
     ]
    }
   ],
   "source": [
    "hpsv2.evaluate(config.ours_hpsv2_path) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
