{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "from pipeline_rf import RectifiedFlowPipeline\n",
    "import random\n",
    "import hpsv2\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "random.seed(2024)\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()\n",
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px')\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/model/stable-diffusion-2-1\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "sd_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instaflow_pipe = RectifiedFlowPipeline.from_pretrained(\"/data/model/instaflow_0_9B_from_sd_1_5\", torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=False) \n",
    "### switch to torch.float32 for higher quality\n",
    "\n",
    "instaflow_pipe.to(device)  ### if GPU is not available, comment this line\n",
    "instaflow_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:09<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "sdxl_turbo_pipe = AutoPipelineForText2Image.from_pretrained(\"/data/model/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "sdxl_turbo_pipe.to(device)\n",
    "sdxl_turbo_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:03<00:00,  1.58it/s]\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:766: FutureWarning: `torch_dtype` is deprecated and will be removed in version 0.27.0. \n",
      "  deprecate(\"torch_dtype\", \"0.27.0\", \"\")\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:769: FutureWarning: `torch_device` is deprecated and will be removed in version 0.27.0. \n",
      "  deprecate(\"torch_device\", \"0.27.0\", \"\")\n"
     ]
    }
   ],
   "source": [
    "lcm_pipe = DiffusionPipeline.from_pretrained(\"/data/model/LCM_Dreamshaper_v7\", safety_checker=None, requires_safety_checker=False)\n",
    "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n",
    "lcm_pipe.to(torch_device=device, torch_dtype=torch.float32)\n",
    "lcm_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text])\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/distill/swift_photo_with_text\"\n",
    "\n",
    "# Initialize empty lists to store images and their names\n",
    "image_list = []\n",
    "image_name_list = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Append the image and its name to the lists\n",
    "        image_list.append(image)\n",
    "        image_name_list.append(filename)\n",
    "\n",
    "# Now, image_list contains PIL Image objects, and image_name_list contains corresponding names\n",
    "avg_score = 0\n",
    "for i in range(len(image_list)):\n",
    "    image = image_list[i]\n",
    "    text = image_name_list[i]\n",
    "    score = get_clip_score(image, text)\n",
    "    avg_score += score\n",
    "    \n",
    "print(f\"AVG CLIP Score: {avg_score/len(image_list)}\") # CLIP score:0.300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "# data = np.load('/data/20231212/SwiftBrush_reproduce_final20231227/val2014_captions.npz')\n",
    "# captions = data['captions'][()]\n",
    "# print(len(captions),captions[0],captions[1])\n",
    "# data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coco30k_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202654 A bicycle replica with a clock as the front wheel. A black Honda motorcycle parked in front of a garage.\n"
     ]
    }
   ],
   "source": [
    "coco_f = open('/data/dataset/coco2014-val/annotations/captions_val2014.json')\n",
    "coco_annotations = json.load(coco_f)\n",
    "captions = []\n",
    "for annotation in coco_annotations['annotations']:\n",
    "    caption = annotation['caption']\n",
    "    captions.append(caption)\n",
    "coco_f.close()\n",
    "print(len(captions),captions[0],captions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 a little green cart filled with assorted suitcases  A woman in an odd outfit on a bed\n"
     ]
    }
   ],
   "source": [
    "captions_30k = random.choices(captions, k=30000)\n",
    "print(len(captions_30k),captions_30k[0],captions_30k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/data/ours_coco30k\"\n",
    "\n",
    "# Initialize empty lists to store images and their names\n",
    "image_list = []\n",
    "image_name_list = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Append the image and its name to the lists\n",
    "        image_list.append(image)\n",
    "        image_name_list.append(filename)\n",
    "\n",
    "# Now, image_list contains PIL Image objects, and image_name_list contains corresponding names\n",
    "avg_score = 0\n",
    "for i in range(len(image_list)):\n",
    "    image = image_list[i]\n",
    "    text = image_name_list[i]\n",
    "    score = get_clip_score(image, text)\n",
    "    avg_score += score\n",
    "    \n",
    "print(f\"AVG CLIP Score: {avg_score/len(image_list)}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smallset_test >>> instaflow:0.26 sd_1_step:0.138 sd_25_step:0.22\n",
    "#instaflow coco30k clip_socre: 0.2580452107747396\n",
    "#sdxl_turbo_4_step coco30K clip_socre: 0.27137984619140626 \n",
    "#sdxl_turbo_1_step coco30K clip_socre: 0.2724981628417969 \n",
    "#lcm coco30k_4_step clip_socre: \n",
    "#lcm coco30k_1_step clip_socre: \n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    # image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "    # image = sd_pipe(prompt=caption, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "    # image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_4_step_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    # image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "    # image = sd_pipe(prompt=caption, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "    # image = lcm_pipe(prompt=caption, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_1_step_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current num: 1000 current avg clip score: 0.26306097412109375\n",
      "A compact home bathroom with toilet, pedestal sink, and tub/shower.\n",
      "current num: 2000 current avg clip score: 0.26297000122070313\n",
      "Street signs from the corner of 8th ave. and 22 3/4 st.\n",
      "current num: 3000 current avg clip score: 0.2630679321289062\n",
      "This tennis pro finds himself resting,  leaning against the advertising graphics/sign.\n",
      "current num: 4000 current avg clip score: 0.2627286529541016\n",
      "A lady is trying to go ice skating/skiing. \n",
      "current num: 5000 current avg clip score: 0.26308931884765624\n",
      "There is a white cow/bull in front of a white building with purple trim.\n",
      "current num: 6000 current avg clip score: 0.26321371459960935\n",
      "a red and yellow train is going past some red lights/train signals\n",
      "A living room with a red/brown area rug, two couches and a large flat screen TV.\n",
      "current num: 7000 current avg clip score: 0.26300069754464284\n",
      "current num: 8000 current avg clip score: 0.26325895690917966\n",
      "current num: 9000 current avg clip score: 0.2632593451605903\n",
      "The rowing crew is rowing down the canal/\n",
      "current num: 10000 current avg clip score: 0.2632309326171875\n",
      "There is a white cow/bull in front of a white building with purple trim.\n",
      "This tennis pro finds himself resting,  leaning against the advertising graphics/sign.\n",
      "current num: 11000 current avg clip score: 0.2631957785866477\n",
      "current num: 12000 current avg clip score: 0.2632307637532552\n",
      "current num: 13000 current avg clip score: 0.2632069091796875\n",
      "current num: 14000 current avg clip score: 0.263126220703125\n",
      "A donut making machine rolling out donuts into a round pan/bowl.\n",
      "current num: 15000 current avg clip score: 0.26297965494791664\n",
      "current num: 16000 current avg clip score: 0.2629269256591797\n",
      "Two people on a moving motorcycle, with a POW/MIA flag on the bike.\n",
      "1 1/2 loaded hot dogs and veggie side\n",
      "current num: 17000 current avg clip score: 0.26297284294577206\n",
      "current num: 18000 current avg clip score: 0.2629339328342014\n",
      "This kitchen has a metal side by side refrigerator/freezer combo.\n",
      "current num: 19000 current avg clip score: 0.26298384817023024\n",
      "current num: 20000 current avg clip score: 0.2629866027832031\n",
      "A kitchen with brown / red painted cabinets\n",
      "A display of several different pastries topped with icing and/or fruit. \n",
      "current num: 21000 current avg clip score: 0.2630198800223214\n",
      "A living room/dining room area with wood furnishings.\n",
      "current num: 22000 current avg clip score: 0.2629354969371449\n",
      "A living room with a red/brown area rug, two couches and a large flat screen TV.\n",
      "A hotel room bathroom with granite vanity and tub/shower.\n",
      "current num: 23000 current avg clip score: 0.262953857421875\n",
      "current num: 24000 current avg clip score: 0.2629546407063802\n",
      "current num: 25000 current avg clip score: 0.2629622412109375\n",
      "current num: 26000 current avg clip score: 0.2629289738581731\n",
      "current num: 27000 current avg clip score: 0.26291719563802085\n",
      "current num: 28000 current avg clip score: 0.2628843340192522\n",
      "Peculiar street sign showing intersection of 23 3/4 St and 8th Ave/CTH D.\n",
      "current num: 29000 current avg clip score: 0.26295126027074356\n",
      "current num: 30000 current avg clip score: 0.262923486328125\n",
      "AVG CLIP Score: 0.262923486328125\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_4_step_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lcm_1_step: 0.21957598673502604\n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_1_step_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hpsv2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------benchmark score ---------------- \n",
      "sdxl_turbo_1_step_hpsv2 anime           28.47 \t 0.1117\n",
      "sdxl_turbo_1_step_hpsv2 concept-art     27.48 \t 0.0845\n",
      "sdxl_turbo_1_step_hpsv2 photo           27.60 \t 0.1805\n",
      "sdxl_turbo_1_step_hpsv2 paintings       27.54 \t 0.1317\n"
     ]
    }
   ],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/sdxl_turbo_1_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "# for style, prompts in all_prompts.items():\n",
    "#     for idx, prompt in enumerate(prompts):\n",
    "#         # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "#         # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "#         image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "#         # TextToImageModel is the model you want to evaluate\n",
    "#         image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "#         # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['yamamoto.']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['yamamoto.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tooth wu, wlop, beeple, and greg rutkowski.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tooth wu, wlop, beeple, and greg rutkowski.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n",
      "-----------benchmark score ---------------- \n",
      "sdxl_turbo_4_step_hpsv2 anime           28.67 \t 0.0960\n",
      "sdxl_turbo_4_step_hpsv2 concept-art     27.83 \t 0.0870\n",
      "sdxl_turbo_4_step_hpsv2 photo           27.86 \t 0.1791\n",
      "sdxl_turbo_4_step_hpsv2 paintings       27.96 \t 0.1141\n"
     ]
    }
   ],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/sdxl_turbo_4_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------benchmark score ---------------- \n",
      "lcm_1_step_hpsv2 anime           22.69 \t 0.1126\n",
      "lcm_1_step_hpsv2 concept-art     22.79 \t 0.1477\n",
      "lcm_1_step_hpsv2 photo           22.92 \t 0.2112\n",
      "lcm_1_step_hpsv2 paintings       22.95 \t 0.1737\n"
     ]
    }
   ],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm_1_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = lcm_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        # image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['yamamoto.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tooth wu, wlop, beeple, and greg rutkowski.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n",
      "-----------benchmark score ---------------- \n",
      "lcm_4_step_hpsv2 anime           26.58 \t 0.1396\n",
      "lcm_4_step_hpsv2 concept-art     26.16 \t 0.0640\n",
      "lcm_4_step_hpsv2 photo           26.24 \t 0.2264\n",
      "lcm_4_step_hpsv2 paintings       26.28 \t 0.1478\n"
     ]
    }
   ],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm_4_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = lcm_pipe(prompt=prompt, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        # image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fid score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
