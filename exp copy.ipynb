{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "from pipeline_rf import RectifiedFlowPipeline\n",
    "import random\n",
    "# import hpsv2\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "random.seed(2024)\n",
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()\n",
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px')\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/model/stable-diffusion-2-1\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "sd_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instaflow_pipe = RectifiedFlowPipeline.from_pretrained(\"/data/model/instaflow_0_9B_from_sd_1_5\", torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=False) \n",
    "### switch to torch.float32 for higher quality\n",
    "\n",
    "instaflow_pipe.to(device)  ### if GPU is not available, comment this line\n",
    "instaflow_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_turbo_pipe = AutoPipelineForText2Image.from_pretrained(\"/data/model/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "sdxl_turbo_pipe.to(device)\n",
    "sdxl_turbo_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm_pipe = DiffusionPipeline.from_pretrained(\"/data/model/LCM_Dreamshaper_v7\", safety_checker=None, requires_safety_checker=False)\n",
    "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n",
    "lcm_pipe.to(torch_device=device, torch_dtype=torch.float32)\n",
    "lcm_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text])\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/distill/swift_photo_with_text\"\n",
    "\n",
    "# Initialize empty lists to store images and their names\n",
    "image_list = []\n",
    "image_name_list = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Append the image and its name to the lists\n",
    "        image_list.append(image)\n",
    "        image_name_list.append(filename)\n",
    "\n",
    "# Now, image_list contains PIL Image objects, and image_name_list contains corresponding names\n",
    "avg_score = 0\n",
    "for i in range(len(image_list)):\n",
    "    image = image_list[i]\n",
    "    text = image_name_list[i]\n",
    "    score = get_clip_score(image, text)\n",
    "    avg_score += score\n",
    "    \n",
    "print(f\"AVG CLIP Score: {avg_score/len(image_list)}\") # CLIP score:0.300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "# data = np.load('/data/20231212/SwiftBrush_reproduce_final20231227/val2014_captions.npz')\n",
    "# captions = data['captions'][()]\n",
    "# print(len(captions),captions[0],captions[1])\n",
    "# data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coco30k_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_f = open('/data/dataset/coco2014-val/annotations/captions_val2014.json')\n",
    "coco_annotations = json.load(coco_f)\n",
    "captions = []\n",
    "for annotation in coco_annotations['annotations']:\n",
    "    caption = annotation['caption']\n",
    "    captions.append(caption)\n",
    "coco_f.close()\n",
    "print(len(captions),captions[0],captions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2024)\n",
    "captions_30k = random.choices(captions, k=30000)\n",
    "print(len(captions_30k),captions_30k[0],captions_30k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step54000-coco.pth\", device)\n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=caption,seed=2024)\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/ours_4_coco30k/\"+str(count)+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step54000-mixcoco.pth\", device)\n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=caption,seed=2024)\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/ours_5_coco30k/\"+str(count)+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smallset_test >>> instaflow:0.26 sd_1_step:0.138 sd_25_step:0.22\n",
    "#instaflow coco30k clip_socre: 0.2580452107747396\n",
    "#sdxl_turbo_4_step coco30K clip_socre: 0.27137984619140626 \n",
    "#sdxl_turbo_1_step coco30K clip_socre: 0.2724981628417969 \n",
    "#lcm coco30k_4_step clip_socre: \n",
    "#lcm coco30k_1_step clip_socre: \n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    # image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "    # image = sd_pipe(prompt=caption, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "    # image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_4_step_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    # image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "    # image = sd_pipe(prompt=caption, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "    # image = lcm_pipe(prompt=caption, num_inference_steps=num_inference_steps, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_1_step_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_4_step_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lcm_1_step: 0.21957598673502604\n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_1_step_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hpsv2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/sdxl_turbo_1_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "# for style, prompts in all_prompts.items():\n",
    "#     for idx, prompt in enumerate(prompts):\n",
    "#         # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "#         # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "#         image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "#         # TextToImageModel is the model you want to evaluate\n",
    "#         image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "#         # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/sdxl_turbo_4_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm_1_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = lcm_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        # image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm_4_step_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = lcm_pipe(prompt=prompt, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        # image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm_1_step_512_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = lcm_pipe(prompt=prompt, height=512, width=512, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm_4_step_512_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = lcm_pipe(prompt=prompt, height=512, width=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fid score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
