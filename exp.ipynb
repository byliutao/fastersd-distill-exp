{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "from pipeline_rf import RectifiedFlowPipeline\n",
    "import random\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import json\n",
    "import generate_swift as gs\n",
    "from generate import generate_single_image, load_model\n",
    "\n",
    "random.seed(2024)\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px')\n",
    "clip_model = clip_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, tokenizer, text_encoder, unet, alphas = gs.load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_final20231227/checkpoints/vsd_global_step4000.pth\")\n",
    "swift_hpsv2_path = \"/data/liutao/data/swift_hpsv2\"\n",
    "swift_coco_path = \"/data/liutao/data/swift_coco30k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints_20240228/vsd_global_step36000_8nis.pth\", device)\n",
    "ours_steps = 8\n",
    "ours_hpsv2_path = \"/data/liutao/data/ours_36k_8i_hpsv2\"\n",
    "ours_coco_path = \"/data/liutao/data/ours_36k_8s_coco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/model/stable-diffusion-2-1\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "sd_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instaflow_pipe = RectifiedFlowPipeline.from_pretrained(\"/data/model/instaflow_0_9B_from_sd_1_5\", torch_dtype=torch.float32, safety_checker=None, requires_safety_checker=False) \n",
    "### switch to torch.float32 for higher quality\n",
    "\n",
    "instaflow_pipe.to(device)  ### if GPU is not available, comment this line\n",
    "instaflow_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_turbo_pipe = AutoPipelineForText2Image.from_pretrained(\"/data/model/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "sdxl_turbo_pipe.to(device)\n",
    "sdxl_turbo_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lcm_pipe = DiffusionPipeline.from_pretrained(\"/data/model/LCM_Dreamshaper_v7\", safety_checker=None, requires_safety_checker=False)\n",
    "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n",
    "lcm_pipe.to(torch_device=device, torch_dtype=torch.float32)\n",
    "lcm_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text], truncate=True)\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image with prompt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/data/ours_coco30k_test\"\n",
    "\n",
    "total_score = 0\n",
    "count = 0\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "        text = os.path.splitext(filename)[0]\n",
    "        score = get_clip_score(image, text)\n",
    "        total_score += score\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"current num:\", count,f\"current AVG CLIP Score: {total_score/count}\")\n",
    "\n",
    "print(f\"AVG CLIP Score: {total_score/count}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coco30k_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 A bicycle replica with a clock as the front wheel. A black Honda motorcycle parked in front of a garage.\n"
     ]
    }
   ],
   "source": [
    "coco_f = open('/data/dataset/coco2014-val/annotations/captions_val2014.json')\n",
    "coco_annotations = json.load(coco_f)\n",
    "captions = []\n",
    "for annotation in coco_annotations['annotations']:\n",
    "    caption = annotation['caption']\n",
    "    captions.append(caption)\n",
    "coco_f.close()\n",
    "random.seed(2024)\n",
    "captions_30k = random.choices(captions, k=30000)\n",
    "print(len(captions_30k),captions[0],captions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load vaild_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = '/data/20231212/SwiftBrush_reproduce_se_parallel/JourneyDB/valid/valid_prompt.jsonl'\n",
    "# Initialize an empty list to store the prompts\n",
    "prompts_list = []\n",
    "with open(jsonl_file_path) as f:\n",
    "    d = json.load(f)\n",
    "    for line in d:\n",
    "        prompts_list.append(line)\n",
    "        \n",
    "random.seed(2024)\n",
    "captions_30k = random.choices(prompts_list, k=30000)\n",
    "print(len(captions_30k),captions_30k[0],captions_30k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current num: 1000 current avg clip score: 0.2615880126953125\n",
      "current num: 2000 current avg clip score: 0.2610256958007813\n",
      "current num: 3000 current avg clip score: 0.260946533203125\n",
      "current num: 4000 current avg clip score: 0.26053231811523436\n",
      "current num: 5000 current avg clip score: 0.2607731689453125\n",
      "current num: 6000 current avg clip score: 0.2605646769205729\n",
      "current num: 7000 current avg clip score: 0.26062793840680804\n",
      "current num: 8000 current avg clip score: 0.26105057525634767\n",
      "current num: 9000 current avg clip score: 0.2608954467773438\n",
      "current num: 10000 current avg clip score: 0.2608013977050781\n",
      "current num: 11000 current avg clip score: 0.2607430586381392\n",
      "current num: 12000 current avg clip score: 0.2606460622151693\n",
      "current num: 13000 current avg clip score: 0.2606663771409255\n",
      "current num: 14000 current avg clip score: 0.26059676252092634\n",
      "current num: 15000 current avg clip score: 0.26063055419921877\n",
      "current num: 16000 current avg clip score: 0.26057498550415037\n",
      "current num: 17000 current avg clip score: 0.26064408964269303\n",
      "current num: 18000 current avg clip score: 0.2606989305284288\n",
      "current num: 19000 current avg clip score: 0.26070269132915297\n",
      "current num: 20000 current avg clip score: 0.26064727478027344\n",
      "current num: 21000 current avg clip score: 0.26070731317429313\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=caption,seed=2024,num_inference_steps=ours_steps)\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        save_name = str(count)+\".jpg\"\n",
    "        image.save(os.path.join(ours_coco_path,save_name))\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    imgs = gs.prompt_to_img_student((vae, tokenizer, text_encoder, unet), caption, seed=2024, alphas=alphas)\n",
    "    imgs = imgs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "    imgs = (imgs * 255).round().astype('uint8')\n",
    "    imgs = gs.image_grid(imgs, grid_size=(-1, 1))\n",
    "    imgs = Image.fromarray(imgs)\n",
    "    score = get_clip_score(imgs, caption)\n",
    "    try:\n",
    "        save_name = str(count)+\".jpg\"\n",
    "        imgs.save(os.path.join(swift_coco_path,save_name))\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/instaflow_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl4_coco30k_test/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl1_coco30k_test/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\", truncation=True).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    print(\"B\")\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/prompt_test_lcm4/\"+text_to_hash(caption)+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    # try:\n",
    "    #     image.save(\"/home/liutao/workspace/data/lcm_1_step_coco30K/\"+caption+\".jpg\")\n",
    "    # except:\n",
    "    #     print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_4_step_512_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_1_step_512_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hpsv2 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpsv2\n",
    "# # Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024,num_inference_steps=ours_steps)\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(ours_path, style, f\"{idx:05d}.jpg\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpsv2.evaluate(ours_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hpsv2\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        imgs = gs.prompt_to_img_student((vae, tokenizer, text_encoder, unet), prompt, seed=2024, alphas=alphas)\n",
    "        imgs = imgs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "        imgs = (imgs * 255).round().astype('uint8')\n",
    "        imgs = gs.image_grid(imgs, grid_size=(-1, 1))\n",
    "        imgs = Image.fromarray(imgs)\n",
    "        imgs.save(os.path.join(swift_hpsv2_path, style, f\"{idx:05d}.jpg\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpsv2.evaluate(swift_hpsv2_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/sdxl4_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm4_hpsv2\"\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = lcm_pipe(prompt=prompt, width=512, height=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fid score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
