{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get repository contents: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/datasets/zhwang/HPDv2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fb80070f880>, 'Connection to huggingface.co timed out. (connect timeout=None)'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "from pipeline_rf import RectifiedFlowPipeline\n",
    "import random\n",
    "import hpsv2\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "\n",
    "random.seed(2024)\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px')\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/model/stable-diffusion-2-1\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "sd_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instaflow_pipe = RectifiedFlowPipeline.from_pretrained(\"/data/model/instaflow_0_9B_from_sd_1_5\", torch_dtype=torch.float16, safety_checker=None, requires_safety_checker=False) \n",
    "### switch to torch.float32 for higher quality\n",
    "\n",
    "instaflow_pipe.to(device)  ### if GPU is not available, comment this line\n",
    "instaflow_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdxl_turbo_pipe = AutoPipelineForText2Image.from_pretrained(\"/data/model/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "sdxl_turbo_pipe.to(device)\n",
    "sdxl_turbo_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:04<00:00,  1.44it/s]\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:766: FutureWarning: `torch_dtype` is deprecated and will be removed in version 0.27.0. \n",
      "  deprecate(\"torch_dtype\", \"0.27.0\", \"\")\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:769: FutureWarning: `torch_device` is deprecated and will be removed in version 0.27.0. \n",
      "  deprecate(\"torch_device\", \"0.27.0\", \"\")\n"
     ]
    }
   ],
   "source": [
    "lcm_pipe = DiffusionPipeline.from_pretrained(\"/data/model/LCM_Dreamshaper_v7\", safety_checker=None, requires_safety_checker=False)\n",
    "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n",
    "lcm_pipe.to(torch_device=device, torch_dtype=torch.float32)\n",
    "lcm_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text])\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image with prompt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current num: 1000 current AVG CLIP Score: 0.23695098876953125\n",
      "current num: 2000 current AVG CLIP Score: 0.23639662170410156\n",
      "current num: 3000 current AVG CLIP Score: 0.23684095255533855\n",
      "current num: 4000 current AVG CLIP Score: 0.23731815338134765\n",
      "current num: 5000 current AVG CLIP Score: 0.23748295288085938\n",
      "current num: 6000 current AVG CLIP Score: 0.23706538899739582\n",
      "current num: 7000 current AVG CLIP Score: 0.2368526393345424\n",
      "current num: 8000 current AVG CLIP Score: 0.23662448120117188\n",
      "current num: 9000 current AVG CLIP Score: 0.23674259440104167\n",
      "current num: 10000 current AVG CLIP Score: 0.23671253051757812\n",
      "current num: 11000 current AVG CLIP Score: 0.23672534734552556\n",
      "current num: 12000 current AVG CLIP Score: 0.2367562764485677\n",
      "current num: 13000 current AVG CLIP Score: 0.23686055814302884\n",
      "current num: 14000 current AVG CLIP Score: 0.23690252685546875\n",
      "current num: 15000 current AVG CLIP Score: 0.23700070393880207\n",
      "current num: 16000 current AVG CLIP Score: 0.2370119743347168\n",
      "current num: 17000 current AVG CLIP Score: 0.23694441492417279\n",
      "current num: 18000 current AVG CLIP Score: 0.23693787977430555\n",
      "current num: 19000 current AVG CLIP Score: 0.236924072265625\n",
      "current num: 20000 current AVG CLIP Score: 0.23690096244812012\n",
      "current num: 21000 current AVG CLIP Score: 0.23689481571742466\n",
      "current num: 22000 current AVG CLIP Score: 0.23691959415782582\n",
      "current num: 23000 current AVG CLIP Score: 0.23692913785188094\n",
      "current num: 24000 current AVG CLIP Score: 0.23693244902292887\n",
      "current num: 25000 current AVG CLIP Score: 0.23692583465576172\n",
      "current num: 26000 current AVG CLIP Score: 0.23686399459838867\n",
      "current num: 27000 current AVG CLIP Score: 0.236824180037887\n",
      "current num: 28000 current AVG CLIP Score: 0.236863618850708\n",
      "current num: 29000 current AVG CLIP Score: 0.23684700985612542\n",
      "AVG CLIP Score: 0.2367912306474689\n"
     ]
    }
   ],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/data/ours_1_coco30k\"\n",
    "\n",
    "total_score = 0\n",
    "count = 0\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "        text = os.path.splitext(filename)[0]\n",
    "        score = get_clip_score(image, text)\n",
    "        total_score += score\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"current num:\", count,f\"current AVG CLIP Score: {total_score/count}\")\n",
    "\n",
    "print(f\"AVG CLIP Score: {total_score/count}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "# data = np.load('/data/20231212/SwiftBrush_reproduce_final20231227/val2014_captions.npz')\n",
    "# captions = data['captions'][()]\n",
    "# print(len(captions),captions[0],captions[1])\n",
    "# data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coco30k_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202654 A bicycle replica with a clock as the front wheel. A black Honda motorcycle parked in front of a garage.\n"
     ]
    }
   ],
   "source": [
    "coco_f = open('/data/dataset/coco2014-val/annotations/captions_val2014.json')\n",
    "coco_annotations = json.load(coco_f)\n",
    "captions = []\n",
    "for annotation in coco_annotations['annotations']:\n",
    "    caption = annotation['caption']\n",
    "    captions.append(caption)\n",
    "coco_f.close()\n",
    "print(len(captions),captions[0],captions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 a little green cart filled with assorted suitcases  A woman in an odd outfit on a bed\n"
     ]
    }
   ],
   "source": [
    "captions_30k = random.choices(captions, k=30000)\n",
    "print(len(captions_30k),captions_30k[0],captions_30k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/instaflow_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smallset_test >>> instaflow:0.26 sd_1_step:0.138 sd_25_step:0.22\n",
    "#instaflow coco30k clip_socre: 0.2580452107747396\n",
    "#sdxl_turbo_4_step coco30K clip_socre: 0.27137984619140626 \n",
    "#sdxl_turbo_1_step coco30K clip_socre: 0.2724981628417969 \n",
    "#lcm coco30k_4_step clip_socre: \n",
    "#lcm coco30k_1_step clip_socre: \n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_4_step_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_1_step_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    # try:\n",
    "    #     image.save(\"/home/liutao/workspace/data/lcm_4_step_coco30K/\"+caption+\".jpg\")\n",
    "    # except:\n",
    "    #     print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    # try:\n",
    "    #     image.save(\"/home/liutao/workspace/data/lcm_1_step_coco30K/\"+caption+\".jpg\")\n",
    "    # except:\n",
    "    #     print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_4_step_512_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current num: 1000 current avg clip score: 0.21697222900390625\n",
      "A compact home bathroom with toilet, pedestal sink, and tub/shower.\n",
      "current num: 2000 current avg clip score: 0.21796038818359376\n",
      "Street signs from the corner of 8th ave. and 22 3/4 st.\n",
      "current num: 3000 current avg clip score: 0.21825511678059895\n",
      "This tennis pro finds himself resting,  leaning against the advertising graphics/sign.\n",
      "current num: 4000 current avg clip score: 0.21803173065185547\n",
      "A lady is trying to go ice skating/skiing. \n",
      "current num: 5000 current avg clip score: 0.21849376831054687\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_1_step_512_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hpsv2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n"
     ]
    }
   ],
   "source": [
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step26000.pth\", device)\n",
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/ours_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024)\n",
    "        # image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------benchmark score ---------------- \n",
      "ours_hpsv2 anime           26.41 \t 0.1411\n",
      "ours_hpsv2 concept-art     26.16 \t 0.0911\n",
      "ours_hpsv2 photo           27.18 \t 0.1781\n",
      "ours_hpsv2 paintings       26.20 \t 0.1378\n"
     ]
    }
   ],
   "source": [
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n",
      "Loading model ...\n",
      "Loading model successfully!\n",
      "-----------benchmark score ---------------- \n",
      "ours_hpsv2 anime           26.40 \t 0.1460\n",
      "ours_hpsv2 concept-art     26.06 \t 0.0990\n",
      "ours_hpsv2 photo           27.15 \t 0.1647\n",
      "ours_hpsv2 paintings       26.19 \t 0.1230\n"
     ]
    }
   ],
   "source": [
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step8000.pth\", device)\n",
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/ours_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        # image = instaflow_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0] \n",
    "        # image = sd_pipe(prompt=prompt, num_inference_steps=25, guidance_scale=0.0).images[0]\n",
    "        image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024)\n",
    "        # image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "        # <image_path> is the folder path to store generated images, as the input of hpsv2.evaluate().\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fid score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
