{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n",
    "from pipeline_rf import RectifiedFlowPipeline\n",
    "import random\n",
    "from diffusers import AutoPipelineForText2Image\n",
    "import json\n",
    "\n",
    "random.seed(2024)\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def text_to_hash(text):\n",
    "    # Create a new SHA-256 hash object\n",
    "    hash_object = hashlib.sha256()\n",
    "\n",
    "    # Update the hash object with the bytes representation of the text\n",
    "    hash_object.update(text.encode('utf-8'))\n",
    "\n",
    "    # Get the hexadecimal representation of the hash\n",
    "    hash_value = hash_object.hexdigest()\n",
    "\n",
    "    return hash_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load('ViT-L/14@336px')\n",
    "clip_model = clip_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/model/stable-diffusion-2-1\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "sd_pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "sd_pipe = sd_pipe.to(device)\n",
    "sd_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instaflow_pipe = RectifiedFlowPipeline.from_pretrained(\"/data/model/instaflow_0_9B_from_sd_1_5\", torch_dtype=torch.float32, safety_checker=None, requires_safety_checker=False) \n",
    "### switch to torch.float32 for higher quality\n",
    "\n",
    "instaflow_pipe.to(device)  ### if GPU is not available, comment this line\n",
    "instaflow_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = instaflow_pipe(prompt=\"caption\", num_inference_steps=1, guidance_scale=0.0).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [06:04<00:00, 52.09s/it] \n"
     ]
    }
   ],
   "source": [
    "sdxl_turbo_pipe = AutoPipelineForText2Image.from_pretrained(\"/data/model/sdxl-turbo\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "sdxl_turbo_pipe.to(device)\n",
    "sdxl_turbo_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:03<00:00,  1.71it/s]\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:766: FutureWarning: `torch_dtype` is deprecated and will be removed in version 0.27.0. \n",
      "  deprecate(\"torch_dtype\", \"0.27.0\", \"\")\n",
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:769: FutureWarning: `torch_device` is deprecated and will be removed in version 0.27.0. \n",
      "  deprecate(\"torch_device\", \"0.27.0\", \"\")\n"
     ]
    }
   ],
   "source": [
    "lcm_pipe = DiffusionPipeline.from_pretrained(\"/data/model/LCM_Dreamshaper_v7\", safety_checker=None, requires_safety_checker=False)\n",
    "# To save GPU memory, torch.float16 can be used, but it may compromise image quality.\n",
    "lcm_pipe.to(torch_device=device, torch_dtype=torch.float32)\n",
    "lcm_pipe.set_progress_bar_config(disable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = lcm_pipe(prompt=\"caption\", height=512, width=512, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clip score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text], truncate=True)\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### image with prompt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/data/ours_coco30k_test\"\n",
    "\n",
    "total_score = 0\n",
    "count = 0\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "        text = os.path.splitext(filename)[0]\n",
    "        score = get_clip_score(image, text)\n",
    "        total_score += score\n",
    "        count += 1\n",
    "        if count % 1000 == 0:\n",
    "            print(\"current num:\", count,f\"current AVG CLIP Score: {total_score/count}\")\n",
    "\n",
    "print(f\"AVG CLIP Score: {total_score/count}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "# data = np.load('/data/20231212/SwiftBrush_reproduce_final20231227/val2014_captions.npz')\n",
    "# captions = data['captions'][()]\n",
    "# print(len(captions),captions[0],captions[1])\n",
    "# data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load coco30k_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202654 A bicycle replica with a clock as the front wheel. A black Honda motorcycle parked in front of a garage.\n"
     ]
    }
   ],
   "source": [
    "coco_f = open('/data/dataset/coco2014-val/annotations/captions_val2014.json')\n",
    "coco_annotations = json.load(coco_f)\n",
    "captions = []\n",
    "for annotation in coco_annotations['annotations']:\n",
    "    caption = annotation['caption']\n",
    "    captions.append(caption)\n",
    "coco_f.close()\n",
    "print(len(captions),captions[0],captions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 a little green cart filled with assorted suitcases  A woman in an odd outfit on a bed\n"
     ]
    }
   ],
   "source": [
    "random.seed(2024)\n",
    "captions_30k = random.choices(captions, k=30000)\n",
    "print(len(captions_30k),captions_30k[0],captions_30k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load vaild_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your JSONL file\n",
    "jsonl_file_path = '/data/20231212/SwiftBrush_reproduce_se_parallel/JourneyDB/valid/valid_prompt.jsonl'\n",
    "# Initialize an empty list to store the prompts\n",
    "prompts_list = []\n",
    "with open(jsonl_file_path) as f:\n",
    "    d = json.load(f)\n",
    "    for line in d:\n",
    "        prompts_list.append(line)\n",
    "print(len(prompts_list),prompts_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2024)\n",
    "captions_30k = random.choices(prompts_list, k=30000)\n",
    "print(len(captions_30k),captions_30k[0],captions_30k[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n",
      "current num: 1000 current avg clip score: 0.2511959228515625\n",
      "current num: 2000 current avg clip score: 0.251618408203125\n",
      "current num: 3000 current avg clip score: 0.2516767578125\n",
      "current num: 4000 current avg clip score: 0.25137857055664065\n",
      "current num: 5000 current avg clip score: 0.251731982421875\n",
      "current num: 6000 current avg clip score: 0.2516671346028646\n",
      "current num: 7000 current avg clip score: 0.25156898716517856\n",
      "current num: 8000 current avg clip score: 0.2519543151855469\n",
      "current num: 9000 current avg clip score: 0.2520390353732639\n",
      "current num: 10000 current avg clip score: 0.25204867553710936\n",
      "current num: 11000 current avg clip score: 0.2521516945578835\n",
      "current num: 12000 current avg clip score: 0.25220989481608075\n",
      "current num: 13000 current avg clip score: 0.2521011915940505\n",
      "current num: 14000 current avg clip score: 0.2520157950265067\n",
      "current num: 15000 current avg clip score: 0.2520740844726562\n",
      "current num: 16000 current avg clip score: 0.25199328231811524\n",
      "current num: 17000 current avg clip score: 0.2520079812442555\n",
      "current num: 18000 current avg clip score: 0.25203227742513024\n",
      "current num: 19000 current avg clip score: 0.2520502961811266\n",
      "current num: 20000 current avg clip score: 0.2520572296142578\n",
      "current num: 21000 current avg clip score: 0.2521612054734003\n",
      "current num: 22000 current avg clip score: 0.25218104969371447\n",
      "current num: 23000 current avg clip score: 0.2521195360266644\n",
      "current num: 24000 current avg clip score: 0.2521024703979492\n",
      "current num: 25000 current avg clip score: 0.25208926513671875\n",
      "current num: 26000 current avg clip score: 0.2521411344088041\n",
      "current num: 27000 current avg clip score: 0.2521242201063368\n",
      "current num: 28000 current avg clip score: 0.2521035461425781\n",
      "current num: 29000 current avg clip score: 0.25211526804956896\n",
      "current num: 30000 current avg clip score: 0.25207930501302084\n",
      "AVG CLIP Score: 0.25207930501302084\n"
     ]
    }
   ],
   "source": [
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step54000-coco.pth\", device)\n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=caption,seed=2024)\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/ours_4_coco30k/\"+str(count)+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current num: 1000 current avg clip score: 0.248734375\n",
      "current num: 2000 current avg clip score: 0.2490621337890625\n",
      "current num: 3000 current avg clip score: 0.2491714884440104\n",
      "current num: 4000 current avg clip score: 0.24889283752441407\n",
      "current num: 5000 current avg clip score: 0.24929661865234376\n",
      "current num: 6000 current avg clip score: 0.24908493041992188\n",
      "current num: 7000 current avg clip score: 0.24897997174944198\n",
      "current num: 8000 current avg clip score: 0.24921016693115233\n",
      "current num: 9000 current avg clip score: 0.24911797417534723\n",
      "current num: 10000 current avg clip score: 0.24897572021484374\n",
      "current num: 11000 current avg clip score: 0.2489399746981534\n",
      "current num: 12000 current avg clip score: 0.24896912638346355\n",
      "current num: 13000 current avg clip score: 0.24885927170973557\n",
      "current num: 14000 current avg clip score: 0.24883101981026787\n",
      "current num: 15000 current avg clip score: 0.2488192626953125\n",
      "current num: 16000 current avg clip score: 0.2488178024291992\n",
      "current num: 17000 current avg clip score: 0.24888511747472428\n",
      "current num: 18000 current avg clip score: 0.24892426215277777\n",
      "current num: 19000 current avg clip score: 0.24887641987047698\n",
      "current num: 20000 current avg clip score: 0.2488623809814453\n",
      "current num: 21000 current avg clip score: 0.2489449259440104\n",
      "current num: 22000 current avg clip score: 0.24889930586381392\n",
      "current num: 23000 current avg clip score: 0.2488626523225204\n",
      "current num: 24000 current avg clip score: 0.24882626342773437\n",
      "current num: 25000 current avg clip score: 0.2488513916015625\n",
      "current num: 26000 current avg clip score: 0.2489241990309495\n",
      "current num: 27000 current avg clip score: 0.24894333676938657\n",
      "current num: 28000 current avg clip score: 0.248924320765904\n",
      "current num: 29000 current avg clip score: 0.24890684772359914\n",
      "current num: 30000 current avg clip score: 0.24883446655273436\n",
      "AVG CLIP Score: 0.24883446655273436\n"
     ]
    }
   ],
   "source": [
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step54000-mixcoco.pth\", device)\n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=caption,seed=2024)\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/ours_5_coco30k/\"+str(count)+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = instaflow_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/instaflow_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smallset_test >>> instaflow:0.26 sd_1_step:0.138 sd_25_step:0.22\n",
    "#instaflow coco30k clip_socre: 0.2580452107747396\n",
    "#sdxl_turbo_4_step coco30K clip_socre: 0.27137984619140626 \n",
    "#sdxl_turbo_1_step coco30K clip_socre: 0.2724981628417969 \n",
    "#lcm coco30k_4_step clip_socre: \n",
    "#lcm coco30k_1_step clip_socre: \n",
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_4_step_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = sdxl_turbo_pipe(prompt=caption, num_inference_steps=1, guidance_scale=0.0).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/sdxl_turbo_1_step_coco30k/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\", truncation=True).images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    print(\"B\")\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/prompt_test_lcm4/\"+text_to_hash(caption)+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    # try:\n",
    "    #     image.save(\"/home/liutao/workspace/data/lcm_1_step_coco30K/\"+caption+\".jpg\")\n",
    "    # except:\n",
    "    #     print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_4_step_512_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_score = 0\n",
    "for case_number, caption in enumerate(captions_30k):\n",
    "    image = lcm_pipe(prompt=caption, height=512, width=512, num_inference_steps=1, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "    score = get_clip_score(image, caption)\n",
    "    try:\n",
    "        image.save(\"/home/liutao/workspace/data/lcm_1_step_512_coco30K/\"+caption+\".jpg\")\n",
    "    except:\n",
    "        print(caption)\n",
    "    total_score += score\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",total_score/count)\n",
    "print(f\"AVG CLIP Score: {total_score/count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hpsv2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get repository contents: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/datasets/zhwang/HPDv2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fc93d768f70>, 'Connection to huggingface.co timed out. (connect timeout=None)'))\n"
     ]
    }
   ],
   "source": [
    "import hpsv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get repository contents: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/datasets/zhwang/HPDv2 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f41deb4bcd0>, 'Connection to huggingface.co timed out. (connect timeout=None)'))\n",
      "[INFO] loading student unet checkpoint\n",
      "Loading model ...\n",
      "Loading model successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------benchmark score ---------------- \n",
      "ours_4_hpsv2 anime           25.76 \t 0.1923\n",
      "ours_4_hpsv2 concept-art     25.41 \t 0.0901\n",
      "ours_4_hpsv2 photo           26.27 \t 0.2177\n",
      "ours_4_hpsv2 paintings       25.42 \t 0.1553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step54000-coco.pth\", device)\n",
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/ours_4_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024)\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading student unet checkpoint\n",
      "Loading model ...\n",
      "Loading model successfully!\n",
      "-----------benchmark score ---------------- \n",
      "ours_5_hpsv2 anime           26.10 \t 0.1443\n",
      "ours_5_hpsv2 concept-art     25.77 \t 0.1013\n",
      "ours_5_hpsv2 photo           26.97 \t 0.1935\n",
      "ours_5_hpsv2 paintings       25.90 \t 0.1088\n"
     ]
    }
   ],
   "source": [
    "import hpsv2\n",
    "from generate import generate_single_image, load_model\n",
    "vae, tokenizer, text_encoder, unet, scheduler, alphas = load_model(\"/data/\", \"/data/20231212/SwiftBrush_reproduce_se_parallel/checkpoints/vsd_global_step54000-mixcoco.pth\", device)\n",
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/ours_5_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = generate_single_image(network=(vae, tokenizer, text_encoder, unet, scheduler),prompt=prompt,seed=2024)\n",
    "        # TextToImageModel is the model you want to evaluate\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sdxl_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['yamamoto.']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['yamamoto.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tooth wu, wlop, beeple, and greg rutkowski.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tooth wu, wlop, beeple, and greg rutkowski.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liutao/miniconda3/envs/instaflow/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------benchmark score ---------------- \n",
      "sdxl4_hpsv2 anime           28.65 \t 0.1071\n",
      "sdxl4_hpsv2 concept-art     27.84 \t 0.0932\n",
      "sdxl4_hpsv2 photo           27.88 \t 0.2003\n",
      "sdxl4_hpsv2 paintings       27.97 \t 0.1341\n"
     ]
    }
   ],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/sdxl4_hpsv2\"\n",
    "# Iterate over the benchmark prompts to generate images\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = sdxl_turbo_pipe(prompt=prompt, num_inference_steps=4, guidance_scale=0.0).images[0]\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lcm4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (80 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['yamamoto.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['tooth wu, wlop, beeple, and greg rutkowski.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model ...\n",
      "Loading model successfully!\n",
      "-----------benchmark score ---------------- \n",
      "lcm4_hpsv2 anime           26.66 \t 0.0932\n",
      "lcm4_hpsv2 concept-art     26.22 \t 0.0851\n",
      "lcm4_hpsv2 photo           26.34 \t 0.1967\n",
      "lcm4_hpsv2 paintings       26.26 \t 0.1468\n"
     ]
    }
   ],
   "source": [
    "# Get benchmark prompts (<style> = all, anime, concept-art, paintings, photo)\n",
    "all_prompts = hpsv2.benchmark_prompts('all') \n",
    "path = \"/home/liutao/workspace/data/lcm4_hpsv2\"\n",
    "for style, prompts in all_prompts.items():\n",
    "    for idx, prompt in enumerate(prompts):\n",
    "        image = lcm_pipe(prompt=prompt, width=512, height=512, num_inference_steps=4, guidance_scale=8.0, lcm_origin_steps=50, output_type=\"pil\").images[0]\n",
    "        image.save(os.path.join(path, style, f\"{idx:05d}.jpg\")) \n",
    "\n",
    "hpsv2.evaluate(path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fid score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
