{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torchmetrics.functional.multimodal import clip_score\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"/data/model/stable-diffusion-2-1-base\"\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, clip_preprocess = clip.load('ViT-B/32')\n",
    "# clip_model = CLIPModel.from_pretrained(\"/data/model/blip-image-captioning-large\")\n",
    "# clip_preprocess = CLIPProcessor.from_pretrained(\"/data/model/blip-image-captioning-large\")\n",
    "clip_model = clip_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_score(image, text):\n",
    "    # Load the pre-trained CLIP model and the image\n",
    "\n",
    "    # Preprocess the image and tokenize the text\n",
    "    image_input = clip_preprocess(image).unsqueeze(0)\n",
    "    text_input = clip.tokenize([text])\n",
    "    \n",
    "    # Move the inputs to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_input = image_input.to(device)\n",
    "    text_input = text_input.to(device)\n",
    "    \n",
    "    # Generate embeddings for the image and text\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_input)\n",
    "        text_features = clip_model.encode_text(text_input)\n",
    "    \n",
    "    # Normalize the features\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    # Calculate the cosine similarity to get the CLIP score\n",
    "    clip_score = torch.matmul(image_features, text_features.T).item()\n",
    "    \n",
    "    return clip_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clip_score_fn = partial(clip_score, model_name_or_path=\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "def calculate_clip_score(images, prompts):\n",
    "    images_int = (images * 255).astype(\"uint8\")\n",
    "    clip_score = clip_score_fn(torch.from_numpy(images_int).permute(0, 3, 1, 2), prompts).detach()\n",
    "    return round(float(clip_score), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a photo of an astronaut riding a horse on mars\"]\n",
    "images = pipe(prompts, num_inference_steps=25, output_type=\"np\").images  \n",
    "\n",
    "sd_clip_score = calculate_clip_score(images, prompts)\n",
    "print(f\"CLIP score: {sd_clip_score}\")\n",
    "# CLIP score: 35.7038"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "image = pipe(prompt, num_inference_steps=25).images[0]  \n",
    "    \n",
    "image\n",
    "print(get_clip_score(image,prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing your images\n",
    "folder_path = \"/home/liutao/workspace/distill/swift_photo_with_text\"\n",
    "\n",
    "# Initialize empty lists to store images and their names\n",
    "image_list = []\n",
    "image_name_list = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Check if the file is an image (you can customize the extension check)\n",
    "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "        # Load the image\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Append the image and its name to the lists\n",
    "        image_list.append(image)\n",
    "        image_name_list.append(filename)\n",
    "\n",
    "# Now, image_list contains PIL Image objects, and image_name_list contains corresponding names\n",
    "avg_score = 0\n",
    "for i in range(len(image_list)):\n",
    "    image = image_list[i]\n",
    "    text = image_name_list[i]\n",
    "    score = get_clip_score(image, text)\n",
    "    avg_score += score\n",
    "    \n",
    "print(f\"AVG CLIP Score: {avg_score/len(image_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .npz file\n",
    "data = np.load('/data/20231212/SwiftBrush_reproduce_final20231227/val2014_captions.npz')\n",
    "captions = data['captions'][()]\n",
    "count = 0\n",
    "avg_score = 0\n",
    "for case_number, caption in enumerate(captions):\n",
    "    image = pipe(caption, num_inference_steps=25).images[0]  \n",
    "    score = get_clip_score(image, caption)\n",
    "    # image.save(\"/home/liutao/workspace/data/sd2_1_base/\"+caption+\".jpg\")\n",
    "    avg_score += score\n",
    "    count += 1\n",
    "    if count >= 30000:\n",
    "        break\n",
    "    if count % 10 == 0:\n",
    "        print(\"current num:\",count,\"current avg clip score:\",avg_score/count)\n",
    "print(f\"AVG CLIP Score: {avg_score/count}\")\n",
    "data.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "instaflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
